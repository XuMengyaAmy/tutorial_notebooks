{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_TrustScore.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f62d3275196a416cad4c624dfcaca5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87fd5e958e2a4321a5ed39a1adc82419",
              "IPY_MODEL_24bb978c829c427f81459a583b1be301",
              "IPY_MODEL_002bee1816d64ee3a9862adbae405cb0"
            ],
            "layout": "IPY_MODEL_77c42bfaa5c74142ab4c3865c08eaf5b"
          }
        },
        "87fd5e958e2a4321a5ed39a1adc82419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee69beeb2ce14174861c6c6345ddc4c5",
            "placeholder": "​",
            "style": "IPY_MODEL_3da48987e0c441e581e4db1b142ac99a",
            "value": ""
          }
        },
        "24bb978c829c427f81459a583b1be301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f29e4e54d1c434f9d944cd55c804061",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af66727b46d44130a51af22da5cd32b5",
            "value": 170498071
          }
        },
        "002bee1816d64ee3a9862adbae405cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a6b5b45d7d4bc18baa4885f96a14ab",
            "placeholder": "​",
            "style": "IPY_MODEL_6d33dc44a8f547b6bf4261d5524a82ca",
            "value": " 170499072/? [00:02&lt;00:00, 78750422.78it/s]"
          }
        },
        "77c42bfaa5c74142ab4c3865c08eaf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee69beeb2ce14174861c6c6345ddc4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da48987e0c441e581e4db1b142ac99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f29e4e54d1c434f9d944cd55c804061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af66727b46d44130a51af22da5cd32b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a6b5b45d7d4bc18baa4885f96a14ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d33dc44a8f547b6bf4261d5524a82ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuMengyaAmy/tutorial_notebooks/blob/main/cifar10_TrustScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ87J3fmDxjE"
      },
      "source": [
        "# CIFAR10:\n",
        "Original CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny77mwOy5rwX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='BalancedLSF Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=1, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9xCxgxhmsn"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=2)\n",
        "\n",
        "model = models.resnet18().to(device)\n",
        "model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "f62d3275196a416cad4c624dfcaca5f2",
            "87fd5e958e2a4321a5ed39a1adc82419",
            "24bb978c829c427f81459a583b1be301",
            "002bee1816d64ee3a9862adbae405cb0",
            "77c42bfaa5c74142ab4c3865c08eaf5b",
            "ee69beeb2ce14174861c6c6345ddc4c5",
            "3da48987e0c441e581e4db1b142ac99a",
            "1f29e4e54d1c434f9d944cd55c804061",
            "af66727b46d44130a51af22da5cd32b5",
            "29a6b5b45d7d4bc18baa4885f96a14ab",
            "6d33dc44a8f547b6bf4261d5524a82ca"
          ]
        },
        "id": "ylEr5myINLRG",
        "outputId": "faf267f5-a8b7-464f-e20e-57072b3f3659"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f62d3275196a416cad4c624dfcaca5f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This cell can be ignored if you wanna use the trained weights from next cell"
      ],
      "metadata": {
        "id": "J3SzNoN6-n0y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsei6ouF6IXy"
      },
      "source": [
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    accuracy = test(model, test_loader)\n",
        "    if accuracy > best_acc:\n",
        "        patience = 0\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), 'best_model_cifar10_lt.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download trained model"
      ],
      "metadata": {
        "id": "byghfr0qOBkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = ['1rDyWMpo1RYa9wFx5gZsXf_mHMwCTr2oz']\n",
        "downloaded = drive.CreateFile({'id':id[0]}) \n",
        "downloaded.GetContentFile('best_model_cifar10_lt.pth.tar')"
      ],
      "metadata": {
        "id": "FRUd5P0JOBBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## confusion matrix: scratch"
      ],
      "metadata": {
        "id": "kWXG58TvNcUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_confusion_matrix(model, testloader):\n",
        "    model.eval()\n",
        "    confusion_matrix = torch.zeros(args.num_classes, args.num_classes)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    acc_per_class = torch.zeros(args.num_classes)\n",
        "    samples_per_class = torch.zeros(args.num_classes)\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(targets.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "            \n",
        "            total += targets.size(0)\n",
        "            correct_matrix = preds.eq(targets)\n",
        "            correct += correct_matrix.sum().item()\n",
        "            for cls in range (args.num_classes):\n",
        "                acc_per_class[cls] += correct_matrix[targets==cls].sum().item()\n",
        "                samples_per_class[cls] += (targets==cls).sum().item()\n",
        "\n",
        "    return correct / total, confusion_matrix, acc_per_class/samples_per_class\n",
        "\n",
        "def get_tp_tn_fp_fn(conf_matrix, nb_classes):\n",
        "    TP = conf_matrix.diag()\n",
        "    for c in range(nb_classes):\n",
        "        idx = torch.ones(nb_classes).byte()\n",
        "        idx[c] = 0\n",
        "        # all non-class samples classified as non-class\n",
        "        TN = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
        "        # all non-class samples classified as class\n",
        "        FP = conf_matrix[idx, c].sum()\n",
        "        # all class samples not classified as class\n",
        "        FN = conf_matrix[c, idx].sum()\n",
        "        \n",
        "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}, acc={}, recall={}, prec={}, total= {}'.format(\n",
        "            c, TP[c], TN, FP, FN,(TP[c]+TN)/(TP[c]+TN+FP+FN), TP[c]/(TP[c]+FN), TP[c]/(TP[c]+FP), (TP[c]+TN+FP+FN) ))\n",
        "    \n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, confusion_matrix, acc_per_class = get_confusion_matrix(model, test_loader)\n",
        "print('confusion matrix:\\n', confusion_matrix)\n",
        "class_wise_acc = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
        "print('per-class accuracy from CM:', class_wise_acc)\n",
        "print('per-class accuracy from scratch:', acc_per_class)\n",
        "print('accuracy-with CM:',class_wise_acc.mean(), ',directly:',acc)\n",
        "\n",
        "get_tp_tn_fp_fn(confusion_matrix, args.num_classes)"
      ],
      "metadata": {
        "id": "yLHhlr2w_6jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrustScore"
      ],
      "metadata": {
        "id": "ZXuzwbLggqlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "\n",
        "\n",
        "class TrustScore:\n",
        "    \"\"\"\n",
        "    Trust Score: a measure of classifier uncertainty based on nearest neighbors.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, k=10, alpha=0.0, filtering=\"none\", min_dist=1e-12):\n",
        "        \"\"\"\n",
        "        k and alpha are the tuning parameters for the filtering,\n",
        "        filtering: method of filtering. option are \"none\", \"density\",\n",
        "        \"uncertainty\"\n",
        "        min_dist: some small number to mitigate possible division by 0.\n",
        "    \"\"\"\n",
        "        self.k = k\n",
        "        self.filtering = filtering\n",
        "        self.alpha = alpha\n",
        "        self.min_dist = min_dist\n",
        "\n",
        "    def filter_by_density(self, X: np.array):\n",
        "        \"\"\"Filter out points with low kNN density.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    Returns:\n",
        "    A subset of the array without points in the bottom alpha-fraction of\n",
        "    original points of kNN density.\n",
        "    \"\"\"\n",
        "        kdtree = KDTree(X)\n",
        "        knn_radii = kdtree.query(X, k=self.k)[0][:, -1]\n",
        "        eps = np.percentile(knn_radii, (1 - self.alpha) * 100)\n",
        "        return X[np.where(knn_radii <= eps)[0], :]\n",
        "\n",
        "    def filter_by_uncertainty(self, X: np.array, y: np.array):\n",
        "        \"\"\"Filter out points with high label disagreement amongst its kNN neighbors.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    Returns:\n",
        "    A subset of the array without points in the bottom alpha-fraction of\n",
        "    samples with highest disagreement amongst its k nearest neighbors.\n",
        "    \"\"\"\n",
        "        neigh = KNeighborsClassifier(n_neighbors=self.k)\n",
        "        neigh.fit(X, y)\n",
        "        confidence = neigh.predict_proba(X)\n",
        "        cutoff = np.percentile(confidence, self.alpha * 100)\n",
        "        unfiltered_idxs = np.where(confidence >= cutoff)[0]\n",
        "        return X[unfiltered_idxs, :], y[unfiltered_idxs]\n",
        "\n",
        "    def fit(self, X: np.array, y: np.array):\n",
        "        \"\"\"Initialize trust score precomputations with training data.\n",
        "    WARNING: assumes that the labels are 0-indexed (i.e.\n",
        "    0, 1,..., n_labels-1).\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    y: corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "        self.n_labels = np.max(y) + 1\n",
        "        self.kdtrees = [None] * self.n_labels\n",
        "        if self.filtering == \"uncertainty\":\n",
        "            X_filtered, y_filtered = self.filter_by_uncertainty(X, y)\n",
        "        for label in range(self.n_labels):\n",
        "            if self.filtering == \"none\":\n",
        "                X_to_use = X[np.where(y == label)[0]]\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "            elif self.filtering == \"density\":\n",
        "                X_to_use = self.filter_by_density(X[np.where(y == label)[0]])\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "            elif self.filtering == \"uncertainty\":\n",
        "                X_to_use = X_filtered[np.where(y_filtered == label)[0]]\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "\n",
        "            if len(X_to_use) == 0:\n",
        "                print(\n",
        "                    \"Filtered too much or missing examples from a label! Please lower \"\n",
        "                    \"alpha or check data.\"\n",
        "                )\n",
        "\n",
        "    def get_score(self, X: np.array, y_pred: np.array):\n",
        "        \"\"\"Compute the trust scores.\n",
        "    Given a set of points, determines the distance to each class.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    y_pred: The predicted labels for these points.\n",
        "    Returns:\n",
        "    The trust score, which is ratio of distance to closest class that was not\n",
        "    the predicted class to the distance to the predicted class.\n",
        "    \"\"\"\n",
        "        d = np.tile(None, (X.shape[0], self.n_labels))\n",
        "        # print('d', d.shape)\n",
        "        for label_idx in range(self.n_labels):\n",
        "            d[:, label_idx] = self.kdtrees[label_idx].query(X, k=2)[0][:, -1]\n",
        "\n",
        "        sorted_d = np.sort(d, axis=1)\n",
        "        d_to_pred = d[range(d.shape[0]), y_pred]\n",
        "        d_to_closest_not_pred = np.where(\n",
        "            sorted_d[:, 0] != d_to_pred, sorted_d[:, 0], sorted_d[:, 1]\n",
        "        )\n",
        "        return d_to_closest_not_pred / (d_to_pred + self.min_dist)\n",
        "\n",
        "\n",
        "class KNNConfidence:\n",
        "    \"\"\"Baseline which uses disagreement to kNN classifier.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, k=10):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.kdtree = KDTree(X)\n",
        "        self.y = y\n",
        "\n",
        "    def get_score(self, X, y_pred):\n",
        "        knn_idxs = self.kdtree.query(X, k=self.k)[1]\n",
        "        knn_outputs = self.y[knn_idxs]\n",
        "        return np.mean(\n",
        "            knn_outputs == np.transpose(np.tile(y_pred, (self.k, 1))), axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JtlgZtWGQr18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate on trustscore (V1):"
      ],
      "metadata": {
        "id": "cmc5xUpyqNqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_trust_model(trust_model, testloader):\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            targets_array = np.array(targets.cpu())\n",
        "            trust_model.fit(inputs_array, targets_array)\n",
        "        \n",
        "    return trust_model\n",
        "\n",
        "def test_trust_score(model, trust_model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    trust_score_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # ========= Get Trust Score ============ #\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            predicted_array = np.array(predicted.cpu())\n",
        "            trust_score = trust_model.get_score(inputs_array, predicted_array)\n",
        "            trust_score_all.extend(trust_score)\n",
        "        \n",
        "        acc = correct / total \n",
        "    return acc, trust_score_all\n",
        "\n",
        "trust_model = TrustScore()\n",
        "trust_model = fit_trust_model(trust_model, test_loader)\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, trust_score_all = test_trust_score(model, trust_model, test_loader)\n",
        "print('Accuracy:', acc)\n",
        "print('Trust Score for First 2 Samples:',trust_score_all[:2] )\n"
      ],
      "metadata": {
        "id": "niX10vjcmVdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate on trustscore (V2):\n",
        "1) pass train loader to .fit(),\n",
        "2) for per batch test data, compute the trust score."
      ],
      "metadata": {
        "id": "scDZVZkXqW1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_processed(loader):\n",
        "    with torch.no_grad():\n",
        "        inputs_array_all = []\n",
        "        targets_array_all = []\n",
        "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            targets_array = np.array(targets.cpu())\n",
        "            inputs_array_all.extend(inputs_array)\n",
        "            targets_array_all.extend(targets_array)\n",
        "\n",
        "        return inputs_array_all, targets_array_all\n",
        "\n",
        "def test_trust_score(model, trust_model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    trust_score_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # ========= Get Trust Score ============ #\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            predicted_array = np.array(predicted.cpu())\n",
        "            trust_score = trust_model.get_score(inputs_array, predicted_array)\n",
        "            trust_score_all.extend(trust_score)\n",
        "        \n",
        "        acc = correct / total \n",
        "    return acc, trust_score_all\n",
        "\n",
        "# Default trust_model\n",
        "trust_model = TrustScore()\n",
        "inputs_array_all, targets_array_all = get_data_processed(train_loader)\n",
        "print('Processed data:',np.array(inputs_array_all).shape, np.array(targets_array_all).shape)\n",
        "trust_model.fit(np.array(inputs_array_all), np.array(targets_array_all))\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, trust_score_all = test_trust_score(model, trust_model, test_loader)\n",
        "print('Accuracy:', acc)\n",
        "print('Default Trust Score for First 2 Samples:',trust_score_all[:2] )\n",
        "\n",
        "# Filter out alpha (0 < alpha < 1) proportion of the training points with\n",
        "# lowest k-NN density when computing trust score.\n",
        "trust_model = TrustScore(k=10, alpha=0.1, filtering=\"density\")\n",
        "# inputs_array_all, targets_array_all = get_data_processed(train_loader)\n",
        "print('Processed data:',np.array(inputs_array_all).shape, np.array(targets_array_all).shape)\n",
        "trust_model.fit(np.array(inputs_array_all), np.array(targets_array_all))\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, trust_score_all = test_trust_score(model, trust_model, test_loader)\n",
        "print('Accuracy:', acc)\n",
        "print('density Trust Score for First 2 Samples:',trust_score_all[:2] )\n",
        "\n",
        "\n",
        "# Filter out alpha (0 < alpha < 1) proportion of the training points with\n",
        "# highest label disagreement amongst its k-NN neighbors.\n",
        "trust_model = TrustScore(k=10, alpha=0.1, filtering=\"disagreement\")\n",
        "# inputs_array_all, targets_array_all = get_data_processed(train_loader)\n",
        "# print('Processed data:',np.array(inputs_array_all).shape, np.array(targets_array_all).shape)\n",
        "trust_model.fit(np.array(inputs_array_all), np.array(targets_array_all))\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, trust_score_all = test_trust_score(model, trust_model, test_loader)\n",
        "print('Accuracy:', acc)\n",
        "print('disagreement Trust Score for First 2 Samples:',trust_score_all[:2] )\n",
        "\n",
        "# uncertainty\n",
        "trust_model = TrustScore(k=10, alpha=0.1, filtering=\"uncertainty\")\n",
        "# inputs_array_all, targets_array_all = get_data_processed(train_loader)\n",
        "# print('Processed data:',np.array(inputs_array_all).shape, np.array(targets_array_all).shape)\n",
        "trust_model.fit(np.array(inputs_array_all), np.array(targets_array_all))\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, trust_score_all = test_trust_score(model, trust_model, test_loader)\n",
        "print('Accuracy:', acc)\n",
        "print('disagreement Trust Score for First 2 Samples:',trust_score_all[:2] )"
      ],
      "metadata": {
        "id": "xdmLgU3mqfPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trust Score V4:\n",
        "1) pass train loader to .fit(),\n",
        "2) prepare all test dataset firstly, then compute the score for entire dataset.\n",
        "\n",
        "Some issue: take a very long time because because of np.tile is sensitive of num_samples"
      ],
      "metadata": {
        "id": "gvjdgCVl4Ptv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_processed(trainloader):\n",
        "    with torch.no_grad():\n",
        "        inputs_array_all = []\n",
        "        targets_array_all = []\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            targets_array = np.array(targets.cpu())\n",
        "            inputs_array_all.extend(inputs_array)\n",
        "            targets_array_all.extend(targets_array)\n",
        "\n",
        "        return inputs_array_all, targets_array_all\n",
        "def test_data_processed(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    inputs_array_all = []\n",
        "    predicted_array_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            inputs_array_all.extend(inputs_array)\n",
        "\n",
        "            predicted_array = np.array(predicted.cpu())\n",
        "            predicted_array_all.extend(predicted_array)\n",
        "\n",
        "        acc = correct / total \n",
        "    return acc, inputs_array_all, predicted_array_all\n",
        "\n",
        "trust_model = TrustScore()\n",
        "inputs_array_all, targets_array_all = get_data_processed(train_loader)\n",
        "print('Processed data:',np.array(inputs_array_all).shape, np.array(targets_array_all).shape) # images-targets pair\n",
        "# To learn a good trust model based on the train loader first\n",
        "trust_model.fit(np.array(inputs_array_all), np.array(targets_array_all)) \n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "# prepare entire test images and entire predicted labels first\n",
        "acc, test_inputs_array_all, test_predicted_array_all = test_data_processed(model,test_loader)\n",
        "print('Process test data', np.array(test_inputs_array_all).shape, np.array(test_predicted_array_all).shape)# images-predicted label pair\n",
        "\n",
        "\n",
        "trust_score_all = trust_model.get_score(np.array(test_inputs_array_all), np.array(test_predicted_array_all)) # will take a very long time because of np.tile is sensitive of num_samples\n",
        "print('Accuracy:', acc)\n",
        "print('Trust Score for First 2 Samples:',trust_score_all[:2] )"
      ],
      "metadata": {
        "id": "k5wxeU_34Ovb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9hWxsNJah1jJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}