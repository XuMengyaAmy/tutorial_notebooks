{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_TrustScore.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuMengyaAmy/tutorial_notebooks/blob/main/cifar10_TrustScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ87J3fmDxjE"
      },
      "source": [
        "# CIFAR10:\n",
        "Original CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny77mwOy5rwX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='BalancedLSF Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=50, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ9xCxgxhmsn"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=4)\n",
        "\n",
        "model = models.resnet18().to(device)\n",
        "model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylEr5myINLRG",
        "outputId": "86c5e197-64c6-467a-f003-360357332d4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrustScore"
      ],
      "metadata": {
        "id": "R3u_SynmBcZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "\n",
        "\n",
        "class TrustScore:\n",
        "    \"\"\"\n",
        "    Trust Score: a measure of classifier uncertainty based on nearest neighbors.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, k=10, alpha=0.0, filtering=\"none\", min_dist=1e-12):\n",
        "        \"\"\"\n",
        "        k and alpha are the tuning parameters for the filtering,\n",
        "        filtering: method of filtering. option are \"none\", \"density\",\n",
        "        \"uncertainty\"\n",
        "        min_dist: some small number to mitigate possible division by 0.\n",
        "    \"\"\"\n",
        "        self.k = k\n",
        "        self.filtering = filtering\n",
        "        self.alpha = alpha\n",
        "        self.min_dist = min_dist\n",
        "\n",
        "    def filter_by_density(self, X: np.array):\n",
        "        \"\"\"Filter out points with low kNN density.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    Returns:\n",
        "    A subset of the array without points in the bottom alpha-fraction of\n",
        "    original points of kNN density.\n",
        "    \"\"\"\n",
        "        kdtree = KDTree(X)\n",
        "        knn_radii = kdtree.query(X, k=self.k)[0][:, -1]\n",
        "        eps = np.percentile(knn_radii, (1 - self.alpha) * 100)\n",
        "        return X[np.where(knn_radii <= eps)[0], :]\n",
        "\n",
        "    def filter_by_uncertainty(self, X: np.array, y: np.array):\n",
        "        \"\"\"Filter out points with high label disagreement amongst its kNN neighbors.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    Returns:\n",
        "    A subset of the array without points in the bottom alpha-fraction of\n",
        "    samples with highest disagreement amongst its k nearest neighbors.\n",
        "    \"\"\"\n",
        "        neigh = KNeighborsClassifier(n_neighbors=self.k)\n",
        "        neigh.fit(X, y)\n",
        "        confidence = neigh.predict_proba(X)\n",
        "        cutoff = np.percentile(confidence, self.alpha * 100)\n",
        "        unfiltered_idxs = np.where(confidence >= cutoff)[0]\n",
        "        return X[unfiltered_idxs, :], y[unfiltered_idxs]\n",
        "\n",
        "    def fit(self, X: np.array, y: np.array):\n",
        "        \"\"\"Initialize trust score precomputations with training data.\n",
        "    WARNING: assumes that the labels are 0-indexed (i.e.\n",
        "    0, 1,..., n_labels-1).\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    y: corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "        self.n_labels = np.max(y) + 1\n",
        "        self.kdtrees = [None] * self.n_labels\n",
        "        if self.filtering == \"uncertainty\":\n",
        "            X_filtered, y_filtered = self.filter_by_uncertainty(X, y)\n",
        "        for label in range(self.n_labels):\n",
        "            if self.filtering == \"none\":\n",
        "                X_to_use = X[np.where(y == label)[0]]\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "            elif self.filtering == \"density\":\n",
        "                X_to_use = self.filter_by_density(X[np.where(y == label)[0]])\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "            elif self.filtering == \"uncertainty\":\n",
        "                X_to_use = X_filtered[np.where(y_filtered == label)[0]]\n",
        "                self.kdtrees[label] = KDTree(X_to_use)\n",
        "\n",
        "            if len(X_to_use) == 0:\n",
        "                print(\n",
        "                    \"Filtered too much or missing examples from a label! Please lower \"\n",
        "                    \"alpha or check data.\"\n",
        "                )\n",
        "\n",
        "    def get_score(self, X: np.array, y_pred: np.array):\n",
        "        \"\"\"Compute the trust scores.\n",
        "    Given a set of points, determines the distance to each class.\n",
        "    Args:\n",
        "    X: an array of sample points.\n",
        "    y_pred: The predicted labels for these points.\n",
        "    Returns:\n",
        "    The trust score, which is ratio of distance to closest class that was not\n",
        "    the predicted class to the distance to the predicted class.\n",
        "    \"\"\"\n",
        "        d = np.tile(None, (X.shape[0], self.n_labels))\n",
        "        for label_idx in range(self.n_labels):\n",
        "            d[:, label_idx] = self.kdtrees[label_idx].query(X, k=2)[0][:, -1]\n",
        "\n",
        "        sorted_d = np.sort(d, axis=1)\n",
        "        d_to_pred = d[range(d.shape[0]), y_pred]\n",
        "        d_to_closest_not_pred = np.where(\n",
        "            sorted_d[:, 0] != d_to_pred, sorted_d[:, 0], sorted_d[:, 1]\n",
        "        )\n",
        "        return d_to_closest_not_pred / (d_to_pred + self.min_dist)\n",
        "\n",
        "\n",
        "class KNNConfidence:\n",
        "    \"\"\"Baseline which uses disagreement to kNN classifier.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, k=10):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.kdtree = KDTree(X)\n",
        "        self.y = y\n",
        "\n",
        "    def get_score(self, X, y_pred):\n",
        "        knn_idxs = self.kdtree.query(X, k=self.k)[1]\n",
        "        knn_outputs = self.y[knn_idxs]\n",
        "        return np.mean(\n",
        "            knn_outputs == np.transpose(np.tile(y_pred, (self.k, 1))), axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "trust_model = TrustScore()\n",
        "trust_score_all = []\n",
        "\n",
        "def test_trust_score(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            # ========= Get Trust Score ============ #\n",
        "            # prepare array format\n",
        "            inputs_array = np.array(inputs.view(inputs.shape[0], -1).cpu())\n",
        "            predicted_array = np.array(predicted.cpu())\n",
        "            targets_array = np.array(targets.cpu())\n",
        "  \n",
        "            trust_model.fit(inputs_array, targets_array)\n",
        "            # Compute trusts score, given (unlabeled) testing examples and (hard) model predictions.\n",
        "            trust_score = trust_model.get_score(inputs_array, predicted_array)\n",
        "            print('trust_score of one-batch data', trust_score)\n",
        "            trust_score_all.extend(trust_score)\n",
        "            # ========================================= #\n",
        "    return correct / total, trust_score\n",
        "_, trust_score_all = test_trust_score(model, test_loader)\n",
        "print('trust_score_all', trust_score_all)"
      ],
      "metadata": {
        "id": "JtlgZtWGQr18",
        "outputId": "249d7d7d-c407-4419-9d1a-2601149f20b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trust_score of one-batch data [0.9790454155459957 1.146953611738629 1.0747447926530873 ...\n",
            " 0.9461359616675329 1.0118351028076993 0.961154426677204]\n",
            "trust_score of one-batch data [1.1113328060887175 0.957859709486549 1.083348010856782 ...\n",
            " 0.9352803107141118 0.9514709920859807 0.9177115721997136]\n",
            "trust_score of one-batch data [0.77244202560451 0.966768756422884 0.9650316192317969 ...\n",
            " 1.1675596334073854 0.992676809336799 0.897426753237556]\n",
            "trust_score of one-batch data [0.8868452277706416 0.991786253692091 0.9289388164302732 ...\n",
            " 1.0513133122607656 0.9519049137792254 0.9334232923796296]\n",
            "trust_score of one-batch data [1.0244807582711073 0.9253632542934236 0.891980937061182 ...\n",
            " 1.2582995940180606 0.7975962835654747 0.9986667422148255]\n",
            "trust_score_all [1.0244807582711073 0.9253632542934236 0.891980937061182 ...\n",
            " 1.2582995940180606 0.7975962835654747 0.9986667422148255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This cell can be ignored if you wanna use the trained weights from next cell"
      ],
      "metadata": {
        "id": "J3SzNoN6-n0y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsei6ouF6IXy"
      },
      "source": [
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    accuracy = test(model, test_loader)\n",
        "    if accuracy > best_acc:\n",
        "        patience = 0\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), 'best_model_cifar10_lt.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download trained model"
      ],
      "metadata": {
        "id": "byghfr0qOBkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = ['1rDyWMpo1RYa9wFx5gZsXf_mHMwCTr2oz']\n",
        "downloaded = drive.CreateFile({'id':id[0]}) \n",
        "downloaded.GetContentFile('best_model_cifar10_lt.pth.tar')"
      ],
      "metadata": {
        "id": "FRUd5P0JOBBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## confusion matrix: scratch"
      ],
      "metadata": {
        "id": "kWXG58TvNcUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_confusion_matrix(model, testloader):\n",
        "    model.eval()\n",
        "    confusion_matrix = torch.zeros(args.num_classes, args.num_classes)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    acc_per_class = torch.zeros(args.num_classes)\n",
        "    samples_per_class = torch.zeros(args.num_classes)\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(targets.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "            \n",
        "            total += targets.size(0)\n",
        "            correct_matrix = preds.eq(targets)\n",
        "            correct += correct_matrix.sum().item()\n",
        "            for cls in range (args.num_classes):\n",
        "                acc_per_class[cls] += correct_matrix[targets==cls].sum().item()\n",
        "                samples_per_class[cls] += (targets==cls).sum().item()\n",
        "\n",
        "    return correct / total, confusion_matrix, acc_per_class/samples_per_class\n",
        "\n",
        "def get_tp_tn_fp_fn(conf_matrix, nb_classes):\n",
        "    TP = conf_matrix.diag()\n",
        "    for c in range(nb_classes):\n",
        "        idx = torch.ones(nb_classes).byte()\n",
        "        idx[c] = 0\n",
        "        # all non-class samples classified as non-class\n",
        "        TN = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
        "        # all non-class samples classified as class\n",
        "        FP = conf_matrix[idx, c].sum()\n",
        "        # all class samples not classified as class\n",
        "        FN = conf_matrix[c, idx].sum()\n",
        "        \n",
        "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}, acc={}, recall={}, prec={}, total= {}'.format(\n",
        "            c, TP[c], TN, FP, FN,(TP[c]+TN)/(TP[c]+TN+FP+FN), TP[c]/(TP[c]+FN), TP[c]/(TP[c]+FP), (TP[c]+TN+FP+FN) ))\n",
        "    \n",
        "\n",
        "model.load_state_dict(torch.load('best_model_cifar10_lt.pth.tar'))\n",
        "acc, confusion_matrix, acc_per_class = get_confusion_matrix(model, test_loader)\n",
        "print('confusion matrix:\\n', confusion_matrix)\n",
        "class_wise_acc = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
        "print('per-class accuracy from CM:', class_wise_acc)\n",
        "print('per-class accuracy from scratch:', acc_per_class)\n",
        "print('accuracy-with CM:',class_wise_acc.mean(), ',directly:',acc)\n",
        "\n",
        "get_tp_tn_fp_fn(confusion_matrix, args.num_classes)"
      ],
      "metadata": {
        "id": "yLHhlr2w_6jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}